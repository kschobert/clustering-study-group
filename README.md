# clustering-study-group
clustering-study-group

# Current focus:
- Bioinformatics Algorithms: An Active Learning Approach
  - [Yeast Genes](https://www.youtube.com/watch?v=mIT0Zql9ITA)
  - [Gene Expression Matrices](https://www.youtube.com/watch?v=ubgY58azZW0)
  - [Clustering as an Optimization Problem](https://www.youtube.com/watch?v=JAC0GqadoiA)  **<-- start here **
  - [From Coin Flipping to k-Means Clustering](https://www.youtube.com/watch?v=3gbIXxutE8E)
  - [Expectation Maximization](https://www.youtube.com/watch?v=P1r4RR1goIU)
  - [Soft k-Means Clustering](https://www.youtube.com/watch?v=fpM0iZTjLhM)
  - [Hierachical Clustering](https://www.youtube.com/watch?v=Aly7YiDjxZs)

## Concepts
- [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN): Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996.[1] It is a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.[2]

# Singular Value Decomposition
- [Gil Strang on SVD](https://www.youtube.com/watch?v=mBcLRGuAFUk)

-[Dimensionality Reduction book chapter from Massive Mining Data Sets](http://infolab.stanford.edu/~ullman/mmds/ch11.pdf) In this chapter we shall explore the idea of dimensionality reduction in more detail. We begin with a discussion of eigenvalues and their use in “principal
component analysis” (PCA). We cover singular-value decomposition, a
more powerful version of UV-decomposition. Finally, because we are always
interested in the largest data sizes we can handle, we look at another form
of decomposition, called CUR-decomposition, which is a variant of singularvalue
decomposition that keeps the matrices of the decomposition sparse if the
original matrix is sparse.
  -[Massive Mining Data Sets book](http://www.mmds.org/)
  -[CS246: Mining Massive Data Sets](http://web.stanford.edu/class/cs246/)

## Spectral clustering
- [spectral-clustering-for-beginners](https://towardsdatascience.com/spectral-clustering-for-beginners-d08b7d25b4d8): 1. Create a similarity graph between our N objects to cluster. 2. Compute the first k eigenvectors of its Laplacian matrix to define a feature vector for each object. 3. Run k-means on these features to separate objects into k classes.
- [A Tutorial on Spectral Clustering by Ulrike von Luxburg](https://www.cs.cmu.edu/~aarti/Class/10701/readings/Luxburg06_TR.pdf)
- [Spectral Clustering Tutorial by Marina Meila](https://www.stat.washington.edu/mmp/Papers/ch2.2-arxiv.pdf)
- [Learning Eigenfunctions Links](http://www.iro.umontreal.ca/~lisa/pointeurs/bengio_eigenfunctions_nc_2004.pdf)
  
## Videos
- [Lecture 30 — The Graph Laplacian Matrix at Stanford](https://www.youtube.com/watch?v=FRZvgNvALJ4)
  
## Resources
- [bioinformaticsalgorithms by Ulrike von Luxburg](http://bioinformaticsalgorithms.com)
- [Canegie Mellon Computational Biology](http://www.cmu.edu/ms-compbio/)
- [Rosalind](http://rosalind.info/problems/locations/)
